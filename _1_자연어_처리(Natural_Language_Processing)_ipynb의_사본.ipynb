{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "_1 자연어 처리(Natural Language Processing).ipynb의 사본",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLv9iQK0HOlX"
      },
      "source": [
        "# 자연어 처리(Natural Language Processing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inehXda7HZhF"
      },
      "source": [
        "* 자연어는 일상 생활에서 사용하는 언어\n",
        "* 자연어 처리는 자연어의 의미를 분석 처리하는 일\n",
        "* 텍스트 분류, 감성 분석, 문서 요약, 번역, 질의 응답, 음성 인식, 챗봇과 같은 응용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W53tu5dXuZR_"
      },
      "source": [
        "## 텍스트 처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gXKJ2H15wF_"
      },
      "source": [
        "s = 'No pain no gain'\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDpKqMwU6G6M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69967c09-6167-468a-e302-01d485bbc02b"
      },
      "source": [
        "'pain' in s\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kXoIXAD6a7J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75cfc67c-b06b-4a11-f2b3-18ffb5b9c2bb"
      },
      "source": [
        "s.split()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['No', 'pain', 'no', 'gain']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71btJnM_6Jp5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb741d89-286b-438f-fda2-4635228768e4"
      },
      "source": [
        "s.split().index('gain')\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRcGQS4h6nnH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "767344a8-711e-4605-e255-247aecbbe613"
      },
      "source": [
        "s[-4:]\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'gain'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igK5VIBI6PMo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c9adb9b2-b85f-4261-c1cf-a802970fb02b"
      },
      "source": [
        "s.split()[1]\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'pain'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjIMwR776SIv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "317fdb3e-2a66-45fc-e061-d0aff134d9b8"
      },
      "source": [
        "s.split()[2][::-1]\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'on'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqk6iozeks2f"
      },
      "source": [
        "s = \"한글도 처리 가능\"\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLusoOBWkvhI"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dy75iFEMkycR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20d5a370-d0f5-4be0-e790-6d4250dcfd68"
      },
      "source": [
        "s.split()\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['한글도', '처리', '가능']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrAyEDZdk1Jf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5a02c285-f833-4412-ff31-496f22a06d8d"
      },
      "source": [
        "s.split()[0]\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'한글도'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--ANCtg2uPwW"
      },
      "source": [
        "## 영어 처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxPYRMKXuiAM"
      },
      "source": [
        "#### 대소문자 통합"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkwMuEpRvFI3"
      },
      "source": [
        "* 대소문자를 통합하지 않는다면 컴퓨터는 같은 단어를 다르게 받아들임\n",
        "* 파이썬의 내장 함수 `lower()`, `upper()`를 통해 간단하게 통합 가능"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9oloj0ivF34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d0a71d6-4840-4c4b-84f4-34b7c82e04c9"
      },
      "source": [
        "s = 'ABCdEfGh'\n",
        "str_lower = s.lower()\n",
        "str_upper = s.upper()\n",
        "print(str_lower,str_upper)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abcdefgh ABCDEFGH\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwDDC4HjPaWn"
      },
      "source": [
        "### 정규화(Normalization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRo-_ohoPdAz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d93c6b4-aa6c-4bc6-bf90-b1f244ca8b65"
      },
      "source": [
        "s = \"I visited UK from US on 22-09-20\"\n",
        "print(s) "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I visited UK from US on 22-09-20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nP3cRP_Pp09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bd9438a-8039-45e0-b8c0-94188ce8cd68"
      },
      "source": [
        "new_s = s.replace(\"UK\",\"United Kingdom\").replace(\"US\",\"United States\").replace(\"-20\",\"-2020\")\n",
        "print(new_s)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I visited United Kingdom from United States on 22-09-2020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwfz36YS1FCU"
      },
      "source": [
        "### 정규표현식"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myBZJRSL1JLE"
      },
      "source": [
        "* 정규 표현식은 특정 문자들을 편리하게 지정하고 추가, 삭제 가능\n",
        "* 데이터 전처리에서 정규 표현식을 많이 사용\n",
        "* 파이썬에서는 정규 표현식을 지원하는 `re` 패키지 제공"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLT-dbGQ1NSe"
      },
      "source": [
        "* 정규 표현식 문법\n",
        "  \n",
        "| 특수문자 | 설명 |\n",
        "| - | - |\n",
        "| `.` | 앞의 문자 1개를 표현 |\n",
        "| `?` | 문자 한개를 표현하나 존재할 수도, 존재하지 않을 수도 있음(0개 또는 1개) |\n",
        "| `*` | 앞의 문자가 0개 이상 |\n",
        "| `+` | 앞의 문자가 최소 1개 이상 |\n",
        "| `^` | 뒤의 문자로 문자열이 시작 |\n",
        "| `\\$` | 앞의 문자로 문자열이 끝남 |\n",
        "| `\\{n\\}` | `n`번만큼 반복 |\n",
        "| `\\{n1, n2\\}` | `n1` 이상, `n2` 이하만큼 반복, n2를 지정하지 않으면 `n1` 이상만 반복 |\n",
        "| `\\[ abc \\]` | 안에 문자들 중 한 개의 문자와 매치, a-z처럼 범위도 지정 가능 |\n",
        "| `\\[ ^a \\]` | 해당 문자를 제외하고 매치 |\n",
        "| `a\\|b` | `a` 또는 `b`를 나타냄 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGv0DLZu1PqE"
      },
      "source": [
        "* 정규 표현식에 자주 사용하는 역슬래시(\\\\)를 이용한 문자 규칙\n",
        "\n",
        "| 문자 | 설명 |\n",
        "| - | - |\n",
        "| `\\\\` | 역슬래시 자체를 의미 |\n",
        "| `\\d` | 모든 숫자를 의미, [0-9]와 동일 |\n",
        "| `\\D` | 숫자를 제외한 모든 문자를 의미, [^0-9]와 동일 |\n",
        "| `\\s` | 공백을 의미, [ \\t\\n\\r\\f\\v]와 동일|\n",
        "| `\\S` | 공백을 제외한 모든 문자를 의미, [^ \\t\\n\\r\\f\\v]와 동일 |\n",
        "| `\\w` | 문자와 숫자를 의미, [a-zA-Z0-9]와 동일 |\n",
        "| `\\W` | 문자와 숫자를 제외한 다른 문자를 의미, [^a-zA-Z0-9]와 동일 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEIH8rWo1bJf"
      },
      "source": [
        "#### match\n",
        "\n",
        "* 컴파일한 정규 표현식을 이용해 문자열이 정규 표현식과 맞는지 검사"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8UD0NCw8UVm"
      },
      "source": [
        "import re\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxzhmTi11wS1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f025c64-41ae-48e0-e2ce-de538d0e9849"
      },
      "source": [
        "check = 'ab.'\n",
        "print(re.match(check,'abc'))\n",
        "print(re.match(check,'c'))\n",
        "print(re.match(check,'ab'))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<re.Match object; span=(0, 3), match='abc'>\n",
            "None\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lICVJ-q_1c_M"
      },
      "source": [
        "#### compile\n",
        "\n",
        "* compile을 사용하면 여러 번 사용할 경우 일반 사용보다 더 빠른 속도를 보임\n",
        "* compile을 통해 정규 표현식을 사용할 경우 `re`가 아닌 컴파일한 객체 이름을 통해 사용해야 함\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDTSl5S31wms",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06b21e3f-997f-401b-be8a-22801fbd7182"
      },
      "source": [
        "import time\n",
        "\n",
        "normal_s_time = time.time()\n",
        "r='ab'\n",
        "for i in range(1000):\n",
        "  re.match(check,'abc')\n",
        "print('일반 사용시 소요 시간: ', time.time() - normal_s_time)\n",
        "\n",
        "compile_s_time = time.time()\n",
        "r = re.compile('ab.')\n",
        "for i in range(1000):\n",
        "  r.match(check)\n",
        "print('컴파일 사용시 소요 시간: ', time.time() - compile_s_time)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "일반 사용시 소요 시간:  0.0020825862884521484\n",
            "컴파일 사용시 소요 시간:  0.0007548332214355469\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6OcEzN81fNV"
      },
      "source": [
        "#### search\n",
        "\n",
        "* match와 다르게, search는 문자열의 전체를 검사"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg1iU0Kg1w70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfd22ef0-cab1-49e2-c143-7569098682a0"
      },
      "source": [
        "check = 'ab?'\n",
        "\n",
        "print(re.search('a', check))\n",
        "print(re.match('kkkab',check))\n",
        "print(re.search('kkkab',check))\n",
        "print(re.match('ab',check))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<re.Match object; span=(0, 1), match='a'>\n",
            "None\n",
            "None\n",
            "<re.Match object; span=(0, 2), match='ab'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nmkvc1741g5P"
      },
      "source": [
        "#### split\n",
        "\n",
        "* 정규표현식에 해당하는 문자열을 기준으로 문자열을 나눔"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bLf5Hgs1xZk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5ac1a04-c90b-4eec-ec14-6700884014ea"
      },
      "source": [
        "r = re.compile(' ')\n",
        "print(r.split('abc abbc abcbab'))\n",
        "\n",
        "r = re.compile('c')\n",
        "print(r.split('abc abbc abcbab'))\n",
        "\n",
        "r = re.compile('[1-9]')\n",
        "print(r.split('s1abc 2v3s 4sss 5a'))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['abc', 'abbc', 'abcbab']\n",
            "['ab', ' abb', ' ab', 'bab']\n",
            "['s', 'abc ', 'v', 's ', 'sss ', 'a']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTor-yg_1h49"
      },
      "source": [
        "#### sub\n",
        "\n",
        "* 정규 표현식과 일치하는 부분을 다른 문자열로 교체\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlqOk9DY1xtF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8b55bfc-2992-4c2c-8cc0-2e6018082fd4"
      },
      "source": [
        "print(re.sub('[a-z]','abcdefg','1'))\n",
        "\n",
        "print(re.sub('[^a-z]','abc defg','1'))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "abc defg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUzY6Vz31i8U"
      },
      "source": [
        "#### findall\n",
        "\n",
        "* 컴파일한 정규 표현식을 이용해 정규 표현식과 맞는 모든 문자(열)을 리스트로 반환\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Osi3UYYv1yHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01c050c5-a72a-4b30-e883-bc6d1a4e369b"
      },
      "source": [
        "print(re.findall('[\\d]','1ab 2cd 3ef 4g'))\n",
        "\n",
        "print(re.findall('[\\W]','!abcd@@#'))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1', '2', '3', '4']\n",
            "['!', '@', '@', '#']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxLLmwJu1kTt"
      },
      "source": [
        "#### finditer\n",
        "\n",
        "* 컴파일한 정규 표현식을 이용해 정규 표현식과 맞는 모든 문자(열)을 `iterator` 객체로 반환\n",
        "* `iterator` 객체를 이용하면 생성된 객체를 하나씩 자동으로 가져올 수 있어 처리가 간편함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NOFG3Hy1ycd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47db9f80-fe23-44cf-d57a-a27dee3280b7"
      },
      "source": [
        "iter1 = re.finditer('[\\d]','1ab 2cd 3ef 4g')\n",
        "print(iter1)\n",
        "for i in iter1:\n",
        "  print(i)\n",
        "\n",
        "iter2 = re.finditer('[\\W]','!abcd@@#')\n",
        "print(iter2)\n",
        "for i in iter2:\n",
        "  print(i)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<callable_iterator object at 0x7fb3fb36a110>\n",
            "<re.Match object; span=(0, 1), match='1'>\n",
            "<re.Match object; span=(4, 5), match='2'>\n",
            "<re.Match object; span=(8, 9), match='3'>\n",
            "<re.Match object; span=(12, 13), match='4'>\n",
            "<callable_iterator object at 0x7fb3fd5db250>\n",
            "<re.Match object; span=(0, 1), match='!'>\n",
            "<re.Match object; span=(5, 6), match='@'>\n",
            "<re.Match object; span=(6, 7), match='@'>\n",
            "<re.Match object; span=(7, 8), match='#'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn4C67oivxxN"
      },
      "source": [
        "### 토큰화(Tokenization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqPZHQZPzFvc"
      },
      "source": [
        "* 특수문자에 대한 처리\n",
        "\n",
        "  + 단어에 일반적으로 사용되는 알파벳, 숫자와는 다르게 특수문자는 별도의 처리가 필요            \n",
        "  + 일괄적으로 단어의 특수문자를 제거하는 방법도 있지만 특수문자가 단어에 특별한 의미를 가질 때 이를 학습에 반영시키지 못할 수도 있음\n",
        "  + 특수문자에 대한 일괄적인 제거보다는 데이터의 특성을 파악하고, 처리를 하는 것이 중요\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrZTwnlTyzYV"
      },
      "source": [
        "* 특정 단어에 대한 토큰 분리 방법\n",
        "\n",
        "  + 한 단어지만 토큰으로 분리할 때 판단되는 문자들로 이루어진 we're, United Kingdom 등의 단어는 어떻게 분리해야 할지 선택이 필요   \n",
        "  + we're은 한 단어이나 분리해도 단어의 의미에 별 영향을 끼치진 않지만 United Kingdom은 두 단어가 모여 특정 의미를 가리켜 분리해선 안됨\n",
        "  + 사용자가 단어의 특성을 고려해 토큰을 분리하는 것이 학습에 유리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9_k9rA6v0KM"
      },
      "source": [
        "#### 단어 토큰화"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t02uSgcSv6IQ"
      },
      "source": [
        "* 파이썬 내장 함수인 `split`을 활용해 단어 토큰화\n",
        "* 공백을 기준으로 단어를 분리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV6CNv1Sv62F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1d17d0c-5aa5-4ac6-fec8-5d4480da37f7"
      },
      "source": [
        "sentence = \"Time is gold\"\n",
        "tokens = [x for x in sentence.split(' ')]\n",
        "tokens"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Time', 'is', 'gold']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_PDCM7YwEi9"
      },
      "source": [
        "* 토큰화는 `nltk` 패키지의 `tokenize` 모듈을 사용해 손쉽게 구현 가능\n",
        "* 단어 토큰화는 `word_tokenize()` 함수를 사용해 구현 가능"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isgXAhV7wEo7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8705d325-f67d-4b5c-866c-d6f498850bf6"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXkXYwoywHAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62393f68-4428-4651-c993-39d3e03624c8"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokens = word_tokenize(sentence)\n",
        "tokens"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Time', 'is', 'gold']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcmIjFyRwJpW"
      },
      "source": [
        "#### 문장 토큰화"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Jkvs6Jov7QF"
      },
      "source": [
        "* 문장 토큰화는 줄바꿈 문자('\\n')를 기준으로 문장을 분리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkJ60tlTv7Ux",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e1b6fec-ed5a-4f12-bc0a-896ae8ab5643"
      },
      "source": [
        "sentences = 'The world is a beautiful book.\\nBut of little use to him who cannot read it.'\n",
        "print(sentences)\n",
        "\n",
        "tokens =[x for x in sentences.split('\\n')]\n",
        "tokens"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The world is a beautiful book.\n",
            "But of little use to him who cannot read it.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The world is a beautiful book.',\n",
              " 'But of little use to him who cannot read it.']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDSxHUHTwVzG"
      },
      "source": [
        "* 문장 토큰화는 `sent_tokenize()` 함수를 사용해 구현 가능"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oR8iHWiwV6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4b22d77-0b6c-4098-fbc9-f7ef05497c87"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "tokens = sent_tokenize(sentences)\n",
        "tokens"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The world is a beautiful book.',\n",
              " 'But of little use to him who cannot read it.']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ca3lOErwbGN"
      },
      "source": [
        "* 문장 토큰화에서는 온점(.)의 처리를 위해 이진 분류기를 사용할 수도 있음\n",
        "* 온점은 문장과 문장을 구분해줄 수도, 문장에 포함된 단어를 구성할 수도 있기 때문에 이를 이진 분류기로 분류해 더욱 좋은 토큰화를 구현할 수도 있음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVPmqCnV2Ydt"
      },
      "source": [
        "#### 정규 표현식을 이용한 토큰화"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hkxXlZb2j0O"
      },
      "source": [
        "* 토큰화 기능을 직접 구현할 수도 있지만 정규 표현식을 이용해 간단하게 구현할 수도 있음\n",
        "* `nltk` 패키지는 정규 표현식을 사용하는 토큰화 도구인 `RegexpTokenizer`를 제공"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KOEXR7R2cyV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecd719eb-81cf-4652-9224-f69e3e841bb8"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "sentence = \"Where there\\'s a will, there\\'s a way\"\n",
        "\n",
        "tokenizer = RegexpTokenizer(\"[\\w]+\")\n",
        "tokens=tokenizer.tokenize(sentence)\n",
        "tokens"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Where', 'there', 's', 'a', 'will', 'there', 's', 'a', 'way']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lhdy85nr2fTt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddab2538-5dd7-484f-dd19-954e88e70afe"
      },
      "source": [
        "tokenizer = RegexpTokenizer(\"[\\s]+\",gaps=True)\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "tokens"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Where', \"there's\", 'a', 'will,', \"there's\", 'a', 'way']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMmCGvTFirsi"
      },
      "source": [
        "#### 케라스를 이용한 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pghLPLfSirOJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "676341d1-c822-40db-d28e-14085b7e41ef"
      },
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "sentence = \"Where there\\'s a will, there\\'s a way\"\n",
        "\n",
        "text_to_word_sequence(sentence)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['where', \"there's\", 'a', 'will', \"there's\", 'a', 'way']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOZj2hkzi9Ki"
      },
      "source": [
        "#### TextBlob을 이용한 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIDyF_gLi84n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06a73244-5ffd-4e0c-fe7b-882eb95ec715"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "sentence = \"Where there\\'s a will, there\\'s a way\"\n",
        "\n",
        "blob = TextBlob(sentence)\n",
        "blob.words"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['Where', 'there', \"'s\", 'a', 'will', 'there', \"'s\", 'a', 'way'])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5Vvqfynlf9Z"
      },
      "source": [
        "#### 기타 토크나이저\n",
        "\n",
        "* WhiteSpaceTokenizer: 공백을 기준으로 토큰화\n",
        "* WordPunktTokenizer: 텍스트를 알파벳 문자, 숫자, 알파벳 이외의 문자 리스트로 토큰화\n",
        "* MWETokenizer: MWE는 Multi-Word Expression의 약자로 'republic of korea'와 같이 여러 단어로 이뤄진 특정 그룹을 한 개체로 취급\n",
        "* TweetTokenizer: 트위터에서 사용되는 문장의 토큰화를 위해서 만들어졌으며, 문장 속 감성의 표현과 감정을 다룸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQh3deRog1It"
      },
      "source": [
        "### n-gram 추출\n",
        "\n",
        "* n-gram은 n개의 어절이나 음절을 연쇄적으로 분류해 그 빈도를 분석\n",
        "* n=1일 때는 unigram, n=2일 때는 bigram, n=3일 때는 trigram으로 불림"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Jj0RiGng7bV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b11361c-a916-4e4f-98bf-949276be901d"
      },
      "source": [
        "from nltk import ngrams\n",
        "\n",
        "sentence = 'There is no royal road to learning'\n",
        "bigram = list(ngrams(sentence.split(),2))\n",
        "print(bigram)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('There', 'is'), ('is', 'no'), ('no', 'royal'), ('royal', 'road'), ('road', 'to'), ('to', 'learning')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sEZ6IfGh2q2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70cccbdc-cdc4-47f2-8952-a1891ee9032f"
      },
      "source": [
        "trigram = list(ngrams(sentence.split(),3))\n",
        "print(trigram)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('There', 'is', 'no'), ('is', 'no', 'royal'), ('no', 'royal', 'road'), ('royal', 'road', 'to'), ('road', 'to', 'learning')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0g1yxT62iMvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c38bc19-7994-4d58-c172-b4485c223c39"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "blob = TextBlob(sentence)\n",
        "blob.ngrams(n=2)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['There', 'is']),\n",
              " WordList(['is', 'no']),\n",
              " WordList(['no', 'royal']),\n",
              " WordList(['royal', 'road']),\n",
              " WordList(['road', 'to']),\n",
              " WordList(['to', 'learning'])]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hkjduP0idli",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "387752bd-356c-4296-c3d1-f19d4aa86f17"
      },
      "source": [
        "blob.ngrams(n=3)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['There', 'is', 'no']),\n",
              " WordList(['is', 'no', 'royal']),\n",
              " WordList(['no', 'royal', 'road']),\n",
              " WordList(['royal', 'road', 'to']),\n",
              " WordList(['road', 'to', 'learning'])]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iRYvPrOZbGQ"
      },
      "source": [
        "### PoS(Parts of Speech) 태깅 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xRajzx4_5ng"
      },
      "source": [
        "* PoS는 품사를 의미하며, PoS 태깅은 문장 내에서 단어에 해당하는 각 품사를 태깅"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo4n1kbt_4Pt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd05aa78-0f94-40c5-8fd6-66675e0f2b60"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk import word_tokenize"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcVS13f8ATMT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9ec3748-1859-489d-90ae-a6b55034ec6c"
      },
      "source": [
        "words = word_tokenize(\"Think like man of action and act like man of thought.\")\n",
        "words"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Think',\n",
              " 'like',\n",
              " 'man',\n",
              " 'of',\n",
              " 'action',\n",
              " 'and',\n",
              " 'act',\n",
              " 'like',\n",
              " 'man',\n",
              " 'of',\n",
              " 'thought',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvoGzfP2AjqJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceb63152-cc66-4e42-c646-c02d2aa504f5"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "nltk.pos_tag(words)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Think', 'VBP'),\n",
              " ('like', 'IN'),\n",
              " ('man', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('action', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('act', 'NN'),\n",
              " ('like', 'IN'),\n",
              " ('man', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('thought', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AaI3nFwB_qL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6d587d2-23be-4a1b-c1a8-10d1a17caccf"
      },
      "source": [
        "nltk.pos_tag(word_tokenize(\"A rolling stone gathers no mass\"))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('A', 'DT'),\n",
              " ('rolling', 'VBG'),\n",
              " ('stone', 'NN'),\n",
              " ('gathers', 'NNS'),\n",
              " ('no', 'DT'),\n",
              " ('mass', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWUanibELmmi"
      },
      "source": [
        "* PoS 태그 리스트"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RGD-0sVAwaV"
      },
      "source": [
        "| Number | Tag | Description | 설명 |\n",
        "| -- | -- | -- | -- |\n",
        "| 1 | `CC` | Coordinating conjunction |\n",
        "| 2 | `CD` | Cardinal number |\n",
        "| 3 | `DT` | Determiner | 한정사\n",
        "| 4 | `EX` | Existential there |\n",
        "| 5 | `FW` | Foreign word | 외래어 |\n",
        "| 6 | `IN` | Preposition or subordinating conjunction | 전치사 또는 종속 접속사 |\n",
        "| 7 | `JJ` | Adjective | 형용사 |\n",
        "| 8 | `JJR` | Adjective, comparative | 헝용사, 비교급 |\n",
        "| 9 | `JJS` | Adjective, superlative | 형용사, 최상급 |\n",
        "| 10 | `LS` | List item marker |\n",
        "| 11 | `MD` | Modal |\n",
        "| 12 | `NN` | Noun, singular or mass | 명사, 단수형 |\n",
        "| 13 | `NNS` | Noun, plural | 명사, 복수형 |\n",
        "| 14 | `NNP` | Proper noun, singular | 고유명사, 단수형 |\n",
        "| 15 | `NNPS` | Proper noun, plural | 고유명사, 복수형 |\n",
        "| 16 | `PDT` | Predeterminer | 전치한정사 |\n",
        "| 17 | `POS` | Possessive ending | 소유형용사 |\n",
        "| 18 | `PRP` | Personal pronoun | 인칭 대명사 |\n",
        "| 19 | `PRP$` | Possessive pronoun | 소유 대명사 |\n",
        "| 20 | `RB` | Adverb | 부사 |\n",
        "| 21 | `RBR` | Adverb, comparative | 부사, 비교급 |\n",
        "| 22 | `RBS` | Adverb, superlative | 부사, 최상급 |\n",
        "| 23 | `RP` | Particle |\n",
        "| 24 | `SYM` | Symbol | 기호\n",
        "| 25 | `TO` | to |\n",
        "| 26 | `UH` | Interjection | 감탄사 |\n",
        "| 27 | `VB` | Verb, base form | 동사, 원형 |\n",
        "| 28 | `VBD` | Verb, past tense | 동사, 과거형 |\n",
        "| 29 | `VBG` | Verb, gerund or present participle | 동사, 현재분사 |\n",
        "| 30 | `VBN` | Verb, past participle | 동사, 과거분사 |\n",
        "| 31 | `VBP` | Verb, non-3rd person singular present | 동사, 비3인칭 단수 |\n",
        "| 32 | `VBZ` | Verb, 3rd person singular present | 동사, 3인칭 단수 |\n",
        "| 33 | `WDT` | Wh-determiner |\n",
        "| 34 | `WP` | Wh-pronoun |\n",
        "| 35 | `WP$` | Possessive wh-pronoun |\n",
        "| 36 | `WRB` | Wh-adverb |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmDuyhjxugFN"
      },
      "source": [
        "### 불용어 제거"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj2rvWzwuy7o"
      },
      "source": [
        "* 영어의 전치사(on, in), 한국어의 조사(을, 를) 등은 분석에 필요하지 않은 경우가 많음\n",
        "* 길이가 짧은 단어, 등장 빈도 수가 적은 단어들도 분석에 큰 영향을 주지 않음\n",
        "* 일반적으로 사용되는 도구들은 해당 단어들을 제거해주지만 완벽하게 제거되지는 않음\n",
        "* 사용자가 불용어 사전을 만들어 해당 단어들을 제거하는 것이 좋음\n",
        "* 도구들이 걸러주지 않는 전치사, 조사 등을 불용어 사전을 만들어 불필요한 단어들을 제거\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjFp9u9quy_t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cde7c1a-a7d9-48e0-8050-485aeebacc08"
      },
      "source": [
        "stop_words = \"on in the\"\n",
        "stop_words = stop_words.split(' ')\n",
        "stop_words"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['on', 'in', 'the']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nKwwtR6uzCr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36d43871-c5c0-42e6-b3f1-9c777665f50d"
      },
      "source": [
        "sentence = \"singer on the stage\"\n",
        "sentence = sentence.split(' ')\n",
        "nouns = []\n",
        "for noun in sentence:\n",
        "  if noun not in stop_words:\n",
        "    nouns.append(noun)\n",
        "\n",
        "nouns"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['singer', 'stage']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykTS2yeMMEXF"
      },
      "source": [
        "* `nltk` 패키지에 불용어 리스트 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlDpO3e-MHwa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5f6638b-1dd1-451f-baa3-982d78626729"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jC408N2MRR0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea7a37ba-580f-4faf-f65f-5123186d965a"
      },
      "source": [
        "stop_words = stopwords.words(\"english\")\n",
        "print(stop_words)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zShCTozjMYWG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bff09a5-6921-489c-ee6f-6fdffa632e17"
      },
      "source": [
        "s = 'If you do not walk today, you will have to run tomorrow.'\n",
        "words = word_tokenize(s)\n",
        "print(words)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['If', 'you', 'do', 'not', 'walk', 'today', ',', 'you', 'will', 'have', 'to', 'run', 'tomorrow', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd_gC78EOPdv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7023102-ce22-4da1-eb8c-e45275ace7c6"
      },
      "source": [
        "no_stopwords = []\n",
        "for w in words:\n",
        "  if w not in stop_words:\n",
        "    no_stopwords.append(w)\n",
        "\n",
        "print(no_stopwords)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['If', 'walk', 'today', ',', 'run', 'tomorrow', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXO6E3E--gQy"
      },
      "source": [
        "### 철자 교정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM6lTU2w-q7A"
      },
      "source": [
        "* 텍스트에 오탈자가 존재하는 경우가 있음\n",
        "* 예를 들어, 단어 'apple'을 'aplpe'과 같이 철자 순서가 바뀌거나 spple 같이 철자가 틀릴 수 있음\n",
        "* 사람이 적절한 추정을 통해 이해하는데는 문제가 없지만, 컴퓨터는 이러한 단어를 그대로 받아들여 처리가 필요\n",
        "* 철자 교정 알고리즘은 이미 개발되어 워드 프로세서나 다양한 서비스에서 많이 적용됨\n",
        "됨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3hUcYaYRwVP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea7e98a2-2125-48ac-f41a-963a0ba2952e"
      },
      "source": [
        "!pip install autocorrect\n",
        "from autocorrect import Speller"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autocorrect\n",
            "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
            "\u001b[?25l\r\u001b[K     |▌                               | 10 kB 15.6 MB/s eta 0:00:01\r\u001b[K     |█                               | 20 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 30 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██                              | 40 kB 3.5 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 51 kB 3.5 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 61 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 71 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 81 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 92 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 102 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 112 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 122 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 133 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 143 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 153 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 163 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 174 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 184 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 194 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 204 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 215 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 225 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 235 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 245 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 256 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 266 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 276 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 286 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 296 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 307 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 317 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 327 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 337 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 348 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 358 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 368 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 378 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 389 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 399 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 409 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 419 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 430 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 440 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 450 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 460 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 471 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 481 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 491 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 501 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 512 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 522 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 532 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 542 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 552 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 563 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 573 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 583 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 593 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 604 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 614 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 622 kB 4.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622382 sha256=a0bb9269ef95e89c44f6a23a5608d61da1ba804755e7c2288e3d3ba95a39f022\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/d4/37/8244101ad50b0f7d9bffd93ce58ed7991ee1753b290923934b\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4H4TaYE7REcI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0070e466-c668-4aa8-b871-b634a235cae5"
      },
      "source": [
        "spell = Speller('en')\n",
        "\n",
        "print(spell('peoplle'))\n",
        "print(spell('peope'))\n",
        "print(spell('peopae'))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "people\n",
            "people\n",
            "people\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO03TM_KSL4V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "196d676c-177b-4e36-b3fd-3ae02b144c92"
      },
      "source": [
        "s = word_tokenize(\"Earlly biird catchess the womm.\")\n",
        "print(s)\n",
        "ss= ' '.join([spell(s) for s in s])\n",
        "print(ss)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Earlly', 'biird', 'catchess', 'the', 'womm', '.']\n",
            "Early bird catches the worm .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4_svyI2myE3"
      },
      "source": [
        "### 언어의 단수화와 복수화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tDE8Aeumxml",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6e9d9ac-6f76-4035-caa0-11e629446bf4"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "words = 'apples bananas oranges'\n",
        "tb = TextBlob(words)\n",
        "\n",
        "print(tb.words)\n",
        "print(tb.words.singularize())"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['apples', 'bananas', 'oranges']\n",
            "['apple', 'banana', 'orange']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLqLAg6Snfi8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e468ecd-3991-4479-bc54-8639628f2cc5"
      },
      "source": [
        "words = 'car train airplane'\n",
        "tb = TextBlob(words)\n",
        "\n",
        "print(tb.words)\n",
        "print(tb.words.pluralize())"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['car', 'train', 'airplane']\n",
            "['cars', 'trains', 'airplanes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-9i7ubNX0mD"
      },
      "source": [
        "### 어간(Stemming) 추출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_28SNykTEbf"
      },
      "source": [
        "import nltk\n",
        "\n",
        "stemmer = nltk.stem.PorterStemmer()"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCwnOPFNTP2v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5c126fb9-a256-4986-c7c1-a85df6987965"
      },
      "source": [
        "stemmer.stem('application')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'applic'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6O5VoWcTQuO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1f8cd2c8-0e0f-4840-baa4-854101177940"
      },
      "source": [
        "stemmer.stem('beginning')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'begin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHvL3FirTUQO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4566a6b1-340d-4856-e6d8-257356a01971"
      },
      "source": [
        "stemmer.stem('catches')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'catch'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B7VXlfMTiAj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c235e5e6-5f4a-4d10-c84c-5b01b2e04f3d"
      },
      "source": [
        "stemmer.stem('education')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'educ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-zauW83XwJ8"
      },
      "source": [
        "### 표제어(Lemmatization) 추출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTHh2W06TlPm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73f8abaa-b9f6-4bc8-a1b3-604b4265474b"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF_jyPwaTzV5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "30a852f1-db5e-49eb-ea84-da369c1dc36e"
      },
      "source": [
        "lemmatizer.lemmatize('application')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'application'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrFilM4jWF9p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "592f668d-e1e7-4867-d861-61b7fffda632"
      },
      "source": [
        "lemmatizer.lemmatize('beginning')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'beginning'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1pwiZSgWV8Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e71f2f11-d5f0-478f-a47a-c26cf9ceabe7"
      },
      "source": [
        "lemmatizer.lemmatize('catches')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'catch'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoKJNaJ8WT2x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "609477df-0875-45bd-915b-9ecc4f276406"
      },
      "source": [
        "lemmatizer.lemmatize('education')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'education'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YauGaSZHWg3R"
      },
      "source": [
        "### 개체명 인식(Named Entity Recognition)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5tcQw9FW8fi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e811846-537f-43b8-e9c4-22a7bd8dc35e"
      },
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqWjHuthXWp4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4460a291-29a7-4c1f-e66c-882d7c4f29dc"
      },
      "source": [
        "s = \"Rome was not built in a day.\"\n",
        "print(s)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rome was not built in a day.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnnBqd-YXkwa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eacc18e0-c86b-48da-e512-c186fdd426de"
      },
      "source": [
        "tags = nltk.pos_tag(word_tokenize(s))\n",
        "print(tags)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Rome', 'NNP'), ('was', 'VBD'), ('not', 'RB'), ('built', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('day', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjnWKrpPYWax",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93834045-1013-4783-d97d-3fc2fad77c60"
      },
      "source": [
        "entities = nltk.ne_chunk(tags, binary = True)\n",
        "print(entities)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S (NE Rome/NNP) was/VBD not/RB built/VBN in/IN a/DT day/NN ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOVaBRHZauv8"
      },
      "source": [
        "### 단어 중의성(Lexical Ambiguity)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9Y7u8Kqawho",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e61b900-f069-4e72-f083-c2d39d1320b3"
      },
      "source": [
        "import nltk\n",
        "from nltk.wsd import lesk\n",
        "\n",
        "s = \"I saw bats.\"\n",
        "\n",
        "print(word_tokenize(s))\n",
        "print(lesk(word_tokenize(s), 'saw'))\n",
        "print(lesk(word_tokenize(s),'bats'))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'saw', 'bats', '.']\n",
            "Synset('saw.v.01')\n",
            "Synset('squash_racket.n.01')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU4T_80TuSdp"
      },
      "source": [
        "## 한국어 처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qeUwjUq3lkH"
      },
      "source": [
        "### 정규표현식"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HSV73e530v8"
      },
      "source": [
        "* 한국어 정규 표현식도 대부분의 문법은 영어 정규 표현식과 같음\n",
        "* 한국어는 자음과 모음이 분리되어 있기 때문에, 문법을 지정할 때는 자음과 모음을 동시에 고려해야 함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKYR7b3U44RM"
      },
      "source": [
        "#### match\n",
        "\n",
        "* 컴파일한 정규 표현식을 이용해 문자열이 정규 표현식과 맞는지 검사"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgwMiWUe4zJN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51afc494-2faf-40ef-ffbd-db53704282ba"
      },
      "source": [
        "import re\n",
        "\n",
        "check = '[ㄱ-ㅎ]+'\n",
        "\n",
        "print(re.match(check, 'ㅎ 안녕하세요.'))\n",
        "print(re.match(check, '안녕하세요. ㅎ'))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<re.Match object; span=(0, 1), match='ㅎ'>\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVxRcA8Z457T"
      },
      "source": [
        "#### search\n",
        "\n",
        "* match와 다르게, search는 문자열의 전체를 검사"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWS4rgxq48N7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90221ba2-2d99-4428-a7cb-6d456b4170d9"
      },
      "source": [
        "check = '[ㄱ-ㅎ:ㅏ-ㅣ]+'\n",
        "\n",
        "print(re.search(check, 'ㄱㅏ 안녕하세요.'))\n",
        "print(re.match(check, '안 ㄱㅏ'))\n",
        "print(re.search(check, '안 ㄱㅏ'))\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<re.Match object; span=(0, 2), match='ㄱㅏ'>\n",
            "None\n",
            "<re.Match object; span=(2, 4), match='ㄱㅏ'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCWlVO4S48Ts"
      },
      "source": [
        "#### sub\n",
        "\n",
        "* 정규 표현식과 일치하는 부분을 다른 문자열로 교체"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Kv_oNRs48fU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e59067f5-9af0-400e-95df-a6a0c37873ca"
      },
      "source": [
        "print(re.sub('[가-힣]','가나다라마바사','1'))\n",
        "print(re.sub('[^가-힣]','가나다라마바사','1'))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "가나다라마바사\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe_7bBBdvlXQ"
      },
      "source": [
        "### 토큰화(Tokenization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aEfI5YHysxv"
      },
      "source": [
        "* 한국어를 학습 데이터를 사용할 때는 언어의 특성으로 인해 추가로 고려해야 할 사항이 존재\n",
        "* 한국어는 띄어쓰기를 준수하지 않아도 의미가 전달되는 경우가 많아 띄어쓰기가 지켜지지 않을 가능성이 존재\n",
        "* 띄어쓰기가 지켜지지 않으면 정상적인 토큰 분리가 이루어지지 않음\n",
        "* 한국어는 형태소라는 개념이 존재해 추가로 고려해주어야만 함\n",
        "* '그는', '그가' 등의 단어들은 같은 의미를 가리키지만 텍스트 처리에서는 다르게 받아들일 수 있어 처리를 해줘야 함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "765_lRMqwlZg"
      },
      "source": [
        "#### 한국어 자연어 처리 konlpy와 형태소 분석기 MeCab 설치\n",
        "\n",
        "* https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CoLF80pwnc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdbe4305-ad19-4afe-958a-6196950b6288"
      },
      "source": [
        "!set -x \\\n",
        "&& pip install konlpy \\\n",
        "&& curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+ pip install konlpy\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 17.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.5)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 46.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.3.0 konlpy-0.6.0\n",
            "+ curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh\n",
            "#! /bin/bash\n",
            "#\n",
            "# Copyright 2019 Team KoNLPy\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "#\n",
            "\n",
            "# Set mecab related variable(s)\n",
            "mecab_dicdir=\"/usr/local/lib/mecab/dic/mecab-ko-dic\"\n",
            "\n",
            "# Exit as soon as we fail\n",
            "set -e\n",
            "\n",
            "# Determine OS\n",
            "os=$(uname)\n",
            "if [[ ! $os == \"Linux\" ]] && [[ ! $os == \"Darwin\" ]]; then\n",
            "    echo \"This script does not support this OS.\"\n",
            "    echo \"Try consulting https://github.com/konlpy/konlpy/blob/master/scripts/mecab.sh\"\n",
            "    exit 0\n",
            "fi\n",
            "\n",
            "# Determine sudo\n",
            "if hash \"sudo\" &>/dev/null; then\n",
            "    sudo=\"sudo\"\n",
            "else\n",
            "    sudo=\"\"\n",
            "fi\n",
            "\n",
            "# Determine python\n",
            "# TODO: Prefer python3 and Respect pyenv\n",
            "python=\"python3\"\n",
            "if hash \"pyenv\" &>/dev/null; then\n",
            "    python=\"python\"\n",
            "fi\n",
            "\n",
            "# Determine python site location are writable.\n",
            "check_python_site_location_is_writable(){\n",
            "    $python - <<EOF\n",
            "import site, os\n",
            "found = False\n",
            "for dir in site.getsitepackages():\n",
            "    if not os.path.isdir(dir):\n",
            "        continue\n",
            "    if os.access(dir, os.W_OK | os.X_OK):\n",
            "        found = True\n",
            "        break\n",
            "print(1 if found else 0)\n",
            "EOF\n",
            "}\n",
            "at_user_site=\"\"\n",
            "if [[ \"$(check_python_site_location_is_writable)\" == \"0\" ]]; then\n",
            "    at_user_site=\"--user\"\n",
            "fi\n",
            "\n",
            "install_mecab_ko(){\n",
            "    cd /tmp\n",
            "    curl -LO https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "    tar zxfv mecab-0.996-ko-0.9.2.tar.gz\n",
            "    cd mecab-0.996-ko-0.9.2\n",
            "    ./configure\n",
            "    make\n",
            "    make check\n",
            "    $sudo make install\n",
            "}\n",
            "\n",
            "install_automake(){\n",
            "    ## install requirement automake1.11\n",
            "    # TODO: if not [automake --version]\n",
            "    if [ \"$os\" == \"Linux\" ]; then\n",
            "        if [ \"$(grep -Ei 'debian|buntu|mint' /etc/*release)\" ]; then\n",
            "            $sudo apt-get update && $sudo apt-get install -y automake\n",
            "        elif [ \"$(grep -Ei 'fedora|redhat' /etc/*release)\" ]; then\n",
            "            $sudo yum install -y automake diffutils make\n",
            "        else\n",
            "            ##\n",
            "            # Autoconf\n",
            "            #\n",
            "            # stage directory\n",
            "            builddir=`mktemp -d` && cd $builddir\n",
            "\n",
            "            # download and extract source\n",
            "            curl -LO http://ftpmirror.gnu.org/autoconf/autoconf-latest.tar.gz\n",
            "            tar -zxvf autoconf-latest.tar.gz\n",
            "            rm autoconf-latest.tar.gz\n",
            "\n",
            "            # configure, make, install --prefix=/usr/local\n",
            "            cd autoconf*\n",
            "            ./configure\n",
            "            make\n",
            "            $sudo make install\n",
            "\n",
            "            # erase stage dir\n",
            "            rm -rf $builddir\n",
            "\n",
            "\n",
            "            ##\n",
            "            # Automake\n",
            "            #\n",
            "            # stage directory\n",
            "            builddir=`mktemp -d` && cd $builddir\n",
            "\n",
            "            # download and extract source\n",
            "            curl -LO http://ftpmirror.gnu.org/automake/automake-1.11.tar.gz\n",
            "            tar -zxvf automake-1.11.tar.gz\n",
            "\n",
            "            # configure, make, install --prefix=/usr/local\n",
            "            cd automake-1.11\n",
            "            ./configure\n",
            "            make\n",
            "            $sudo make install\n",
            "\n",
            "            # erase stage dir\n",
            "            rm -rf $builddir\n",
            "        fi\n",
            "\n",
            "    elif [ \"$os\" == \"Darwin\" ]; then\n",
            "        if [[ $(command -v brew) == \"\" ]]; then\n",
            "            echo \"This script require Homebrew!\"\n",
            "            echo \"Try https://brew.sh/\"\n",
            "            exit 0\n",
            "        fi\n",
            "        brew install automake\n",
            "    fi\n",
            "}\n",
            "\n",
            "install_mecab_ko_dic(){\n",
            "    echo \"Install mecab-ko-dic\"\n",
            "    cd /tmp\n",
            "    curl -LO https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "    tar -zxvf mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "    cd mecab-ko-dic-2.1.1-20180720\n",
            "    ./autogen.sh\n",
            "    ./configure\n",
            "    if [[ $os == \"Linux\" ]]; then\n",
            "        mecab-config --libs-only-L | $sudo tee /etc/ld.so.conf.d/mecab.conf  # XXX: Resolve #271, #182, #133\n",
            "        $sudo ldconfig\n",
            "    fi\n",
            "    make\n",
            "    $sudo sh -c 'echo \"dicdir=/usr/local/lib/mecab/dic/mecab-ko-dic\" > /usr/local/etc/mecabrc'\n",
            "    $sudo make install\n",
            "}\n",
            "\n",
            "install_mecab_python(){\n",
            "    pushd /tmp\n",
            "    if [[ ! -d \"mecab-python-0.996\" ]]; then\n",
            "        git clone https://bitbucket.org/eunjeon/mecab-python-0.996.git\n",
            "    fi\n",
            "    popd\n",
            "    if [[ \"$os\" == \"Darwin\" ]]; then\n",
            "        CFLAGS=-stdlib=libc++ $python -m pip install $at_user_site /tmp/mecab-python-0.996\n",
            "    else\n",
            "        # the gcc compiler has no such commandline option as -stdilb, so let's not use it. See discussion on #391.\n",
            "        $python -m pip install $at_user_site /tmp/mecab-python-0.996\n",
            "    fi\n",
            "}\n",
            "\n",
            "\n",
            "if ! hash \"automake\" &>/dev/null; then\n",
            "    echo \"Installing automake (A dependency for mecab-ko)\"\n",
            "    install_automake\n",
            "fi\n",
            "\n",
            "if hash \"mecab\" &>/dev/null; then\n",
            "    echo \"mecab-ko is already installed\"\n",
            "else\n",
            "    echo \"Install mecab-ko\"\n",
            "    install_mecab_ko\n",
            "fi\n",
            "\n",
            "if [[ -d $mecab_dicdir ]]; then\n",
            "    echo \"mecab-ko-dic is already installed\"\n",
            "else\n",
            "    echo \"Install mecab-ko-dic\"\n",
            "    install_mecab_ko_dic\n",
            "fi\n",
            "\n",
            "if [[ $($python -c 'import pkgutil; print(1 if pkgutil.find_loader(\"MeCab\") else 0)') == \"1\" ]]; then\n",
            "    echo \"mecab-python is already installed\"\n",
            "else\n",
            "    echo \"Install mecab-python\"\n",
            "    install_mecab_python\n",
            "fi\n",
            "\n",
            "echo \"Done.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJhzBOckvqr9"
      },
      "source": [
        "#### 단어 토큰화"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-34oqwZwsC3"
      },
      "source": [
        "* 한국어는 공백으로 단어를 분리해도 조사, 접속사 등이 남아 분석에 어려움이 있음\n",
        "* 이를 해결해주는 한국어 토큰화는 조사, 접속사를 분리해주거나 제거\n",
        "* 한국어 토큰화를 사용하기 위해선 `konlpy`와 `mecab`이라는 라이브러리가 필요"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8m9Inqu0wrd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "outputId": "df2e812d-214c-4e6d-abe0-69b6c6afc539"
      },
      "source": [
        "# 정상 동작 확인\n",
        "from konlpy.tag import Okt, Mecab\n",
        "\n",
        "okt = Okt()\n",
        "mecab = Mecab()\n",
        "tagger = Mecab()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/konlpy/tag/_mecab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dicpath)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-d %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdicpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s/data/tagset/mecab.json'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstallpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Tagger' is not defined",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-b23b2bd7f997>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mokt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOkt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmecab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMecab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMecab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/konlpy/tag/_mecab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dicpath)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The MeCab dictionary does not exist at \"%s\". Is the dictionary correctly installed?\\nYou can also try entering the dictionary path when initializing the Mecab class: \"Mecab(\\'/some/dic/path\\')\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdicpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mNameError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Install MeCab in order to use it: http://konlpy.org/en/latest/install/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Install MeCab in order to use it: http://konlpy.org/en/latest/install/"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwoU6RioB14R"
      },
      "source": [
        "sentence = \"언제나 현재에 집중할 수 있다면 행복할것이다.\"\n",
        "tagger.pos(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKg9p-SlwyRe"
      },
      "source": [
        "* 토큰화만 실행할 때는 `tagger.morphs()`라는 함수를 이용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ist7zW_RwvcO"
      },
      "source": [
        "tagger.morphs(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7u6-78hw0gD"
      },
      "source": [
        "* 형태소만 사용하고 싶을 때는 `tagger.nouns()`라는 함수를 이용해 조사, 접속사 등을 제거 가능"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1D_Bg0gewwm9"
      },
      "source": [
        "tagger.nouns(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThPaMDOGvsiW"
      },
      "source": [
        "#### 문장 토큰화"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okKRCWhdxMbH"
      },
      "source": [
        "* 한국어 문장을 토큰화할 때는 kss(korean sentence splitter) 라이브러리 이용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOKstGpcxv5-"
      },
      "source": [
        "!pip install kss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1VGyGg5yN-d"
      },
      "source": [
        "* 라이브러리를 이용해도 한국어에는 전치 표현이 존재해 제대로 토큰화가 안됨\n",
        "* 좀 더 나은 학습을 위해 사용자는 해당 부분을 따로 처리해주어야만 함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIlTDBv7xMg7"
      },
      "source": [
        "import kss\n",
        "\n",
        "text = '진짜? 내일 뭐하지. 이렇게 애매모호한 문장도? 밥은 먹었어? 나는...'\n",
        "print(kss.split_sentences(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAq7fR4r7cpA"
      },
      "source": [
        "#### 정규 표현식을 이용한 토큰화"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkCInLL67l9M"
      },
      "source": [
        "* 한국어도 정규 표현식을 이용해 토큰화 가능"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lyscjy5a7c4r"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "sentence = '안녕하세요 ㅋㅋ 저는 자연어 처리(Natural Language Processing)를ㄹ!! 배우고 있습니다.'\n",
        "\n",
        "tokenizer = RegexpTokenizer(\"[가-힣]+\")\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmXhfXCC7c0u"
      },
      "source": [
        "tokenizer = RegexpTokenizer('[ㄱ-ㅎ]+',gaps=True)\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZdfxeFsn25k"
      },
      "source": [
        "#### 케라스를 이용한 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_Zn15pNn25o"
      },
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "sentence = \"성공의 비결은 단 한 가지, 잘할 수 있는 일에 광적으로 집중하는 것이다.\"\n",
        "\n",
        "text_to_word_sequence(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzEFiZt7n25z"
      },
      "source": [
        "#### TextBlob을 이용한 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgzHxLPyn252"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "blob = TextBlob(sentence)\n",
        "blob.words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xaQF79AaKiN"
      },
      "source": [
        "## Bag of Words(BoW)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-FpBEoS3CBY"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = ['Think like a man of action and act like man of thought.']\n",
        "\n",
        "vector = CountVectorizer()\n",
        "bow = vector.fit_transform(corpus)\n",
        "\n",
        "print(bow.toarray())\n",
        "print(vector.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIjY4WXr48FE"
      },
      "source": [
        "vector = CountVectorizer(stop_words='english')\n",
        "bow = vector.fit_transform(corpus)\n",
        "\n",
        "print(bow.toarray())\n",
        "print(vector.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMeaVigC4eE6"
      },
      "source": [
        "corpus = [\"평생 살 것처럼 꿈을 꾸어라. 그리고 내일 죽을 것처럼 오늘을 살아라.\"]\n",
        "\n",
        "vector = CountVectorizer()\n",
        "bow = vector.fit_transform(corpus)\n",
        "\n",
        "print(bow.toarray())\n",
        "print(vector.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2ud_G3LqIpw"
      },
      "source": [
        "import re\n",
        "from konlpy.tag import Mecab\n",
        "tagger = Mecab()\n",
        "\n",
        "corpus = [\"평생 살 것처럼 꿈을 꾸어라. 그리고 내일 죽을 것처럼 오늘을 살아라.\"]\n",
        "tokens = tagger.morphs(re.sub(\"(\\.)\",\"\",corpus))\n",
        "\n",
        "vocab = []\n",
        "bow = []\n",
        "\n",
        "for tok in tokens:\n",
        "  if tok not in vocab.keys():\n",
        "    vocab[tok] = len(vocab)\n",
        "    bow.insert(len(vocab)-1,1)\n",
        "  else:\n",
        "    index = vocab.get(tok)\n",
        "    bow[index] = bow[index]+1\n",
        "\n",
        "print(bow)\n",
        "print(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5n2gkHk17NyC"
      },
      "source": [
        "## 문서 단어 행렬(DTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F87rTUZs7S2p"
      },
      "source": [
        "* 문서 단어 행렬(Document-Term Matrix)은 문서에 등장하는 여러 단어들의 빈도를 행렬로 표현\n",
        "* 각 문서에 대한 BoW를 하나의 행렬로 표현한 것"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4pNYz6V7SO9"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\"Think like a man of action and act like man of thought.\",\n",
        "          \"Try not to become a man of success but rathe try to become a man of value.\",\n",
        "          \"Give me liberty, or give me death\"]\n",
        "\n",
        "vector = CountVectorizer(stop_words=\"english\")\n",
        "bow = vector.fit_transform(corpus)\n",
        "\n",
        "print(bow.toarray())\n",
        "print(vector.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "columns = []\n",
        "for k,v in sorted(vector.vocabulary_.items(), key=lambda item:item[1]):\n",
        "  columns.append(k)\n",
        "\n",
        "df = pd.DataFrame(bow.toarray(),columns=columns)\n",
        "df"
      ],
      "metadata": {
        "id": "_rHAN72RGf5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z-bPw94FTGb"
      },
      "source": [
        "## 어휘 빈도-문서 역빈도(TF-IDF) 분석"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-0f-hz5Fgai"
      },
      "source": [
        "* 어휘 빈도-문서 역빈도(TF-IDF; Term Frequency-Inverse Docunment Frequency)는 단순히 빈도수가 높은 단어가 핵심어가 아닌, 특정 문서에서만 집중적으로 등장할 때 해당 단어가 문서의 주제를 잘 담고 있는 핵심어라고 가정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3kEwiZBJSSB"
      },
      "source": [
        "* 특정 문서에서 특정단어가 많이 등장하고 그 단어가 다른 문서에서 적게 등장할 때, 그 단어를 특정 문서의 핵심어로 간주\n",
        "* 어휘 빈도-문서 역빈도는 어휘 빈도와 역문서 빈도를 곱해 계산 가능"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7wTnMY8JEDC"
      },
      "source": [
        "* **어휘 빈도**는 특정 문서에서 특정 단어가 많이 등장하는 것을 의미\n",
        "\n",
        "$$ tf_{x,y} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8289-mSJF1p"
      },
      "source": [
        "* **역문서 빈도**는 다른 문서에서 등장하지 않는 단어 빈도를 의미\n",
        "\n",
        "$$ log(N/df_x) $$      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MyZYLgRJH8R"
      },
      "source": [
        "* **어휘 빈도-문서 역빈도**는 다음과 같이 표현\n",
        "\n",
        "$$ W_{x,y} = tf_{x,y} * log(N/df_x) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv_23ugcKMjS"
      },
      "source": [
        "* tf-idf를 편리하게 계산하기 위해 `scikit-learn`의 `tfidfvectorizer`를 이용\n",
        "* 앞서 계산한 단어 빈도 수를 입력하여 tf-idf로 변환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH25kuy-J8cj"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(stop_words = 'english').fit(corpus)\n",
        "\n",
        "print(tfidf.transform(corpus).toarray())\n",
        "print(tfidf.vocabulary_) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoGurU81KqYD"
      },
      "source": [
        "* 좀 더 편리하게 확인하기 위해 데이터프레임으로 변환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSjYYeDBKGEr"
      },
      "source": [
        "import pandas as pd\n",
        "columns = []\n",
        "for k,v in sorted(tfidf.vocabulary_.items(), key=lambda item:item[1]):\n",
        "  columns.append(k)\n",
        "\n",
        "pd.DataFrame(tfidf.transform(corpus).toarray(),columns=columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "i5FmPQH-jV-B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}